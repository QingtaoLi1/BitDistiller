description: Simple Amulet job on Singularity

target:
  service: sing
  name: msrresrchvc
  workspace_name: dca-singularity

environment:
  image: amlt-sing/acpt-torch2.7.1-py3.10-cuda12.6-ubuntu22.04
  setup:
    - pwd
    - sudo apt-get update
    - sudo apt-get install -y vim
    - pip install uv
    - uv venv venv_bd --python 3.11
    - source venv_bd/bin/activate
    - ls -l
    - uv pip install --upgrade pip setuptools packaging wheel ninja
    - cd /scratch/amlt_code
    - uv pip install -r ./requirement_20250807_fsdp.txt
    - uv pip install /mnt/external/flash_attn-2.8.3+cu12torch2.8cxx11abiFALSE-cp311-cp311-linux_x86_64.whl

    - echo -e "alias ll='ls -al'" >> ~/.bashrc

    - cd /scratch/amlt_code/scripts/code_modify
    - chmod +x ./modify*.sh
    - ./modify_for_bd_train_fsdp.sh

storage:
  input:
    storage_account_name: dcasingularity4556773921
    container_name: qingtaoli
    mount_dir: /mnt/input
  output:
    storage_account_name: dcasingularity4556773921
    container_name: qingtaoli
    mount_dir: /mnt/output
  external:
    storage_account_name: dcasingularity4556773921
    container_name: qingtaoli
    mount_dir: /mnt/external

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: ./

data:
  storage_id: external


jobs:
# - name: bd_train_8xA100-NvLink_qwen3_14b_V1_ctx16384_topk512_ranking32
#   # sku: NC_A100_v4:G8
#   # sku: 1x80G8-A100-IB-NvLink
#   sku: 1x80G8-A100-NvLink
#   # sku: 1x80G4-A100
#   identity: managed
#   submit_args:
#     env:
#       _AZUREML_SINGULARITY_JOB_UAI: "/subscriptions/656a79af-6a27-4924-ad92-9221860e3bba/resourceGroups/dca-core/providers/Microsoft.ManagedIdentity/userAssignedIdentities/dca-core-identity"
#   command:
#     - cd scripts/train
#     - chmod +x ./*/train*.sh
#     - export NCCL_DEBUG=INFO
#     - export TORCH_NCCL_ASYNC_ERROR_HANDLING=1

#     ### V1.0 summary
#     - cd fsdp
#     - NCCL_DEBUG=INFO ./train_gpu8_ctx16k.sh /mnt/external/models/Qwen/Qwen3-14B /mnt/external/checkpoints/Qwen/Qwen3-14B/nemotron_code_cakld_ctx16384_A100/ /mnt/external/data/nemotron/code/nemotron-sft-code_500K_block_0_repeat4.jsonl /mnt/external/checkpoints/Qwen/Qwen3-14B/nemotron_code_cakld_ctx16384_A100/logs/ /mnt/external/data/clip_cache/Qwen/Qwen3-14B/int2-g64-code_nemotron.pt 16384 512

#     - cd /mnt/external
#     - nohup python keep.py --gpus=8 --interval=0.2 >/dev/null 2>&1 &
#     - sleep 100000000
#   tags: ["Debug:True"]
#   priority: High
#   azml_int: True

- name: bd_train_8xA100-IB_qwen3_14b_V1_ctx16384_topk512_ranking32_A
  sku: 1x80G8-A100-IB-NvLink
  identity: managed
  submit_args:
    env:
      _AZUREML_SINGULARITY_JOB_UAI: "/subscriptions/656a79af-6a27-4924-ad92-9221860e3bba/resourceGroups/dca-core/providers/Microsoft.ManagedIdentity/userAssignedIdentities/dca-core-identity"
  command:
    - cd scripts/train
    - chmod +x ./*/train*.sh
    - export NCCL_DEBUG=INFO
    - export TORCH_NCCL_ASYNC_ERROR_HANDLING=1

    ### V1.0 summary
    - cd fsdp
    - NCCL_DEBUG=INFO ./train_gpu8_ctx16k.sh /mnt/external/models/Qwen/Qwen3-14B /mnt/external/checkpoints/Qwen/Qwen3-14B/nemotron_code_cakld_ctx16384_H100_top512_ranking32_A/ /mnt/external/data/nemotron/code/nemotron-sft-code_500K_block_0_repeat4.jsonl /mnt/external/checkpoints/Qwen/Qwen3-14B/nemotron_code_cakld_ctx16384_H100_top512_ranking32_A/logs/ /mnt/external/data/clip_cache/Qwen/Qwen3-14B/int2-g64-code_nemotron.pt 16384 512

    - cd /mnt/external
    - nohup python keep.py --gpus=8 --interval=0.2 >/dev/null 2>&1 &
    - sleep 100000000
  tags: ["Debug:True"]
  priority: High
  azml_int: True
